{"cells":[{"cell_type":"markdown","metadata":{"id":"BKUrgt73idGJ"},"source":["This code imports the NLTK and SpaCy libraries and prints their versions.\n","It's a basic setup for working with natural language processing tasks in Python."]},{"cell_type":"markdown","metadata":{"id":"Ug3IvsKch9FG"},"source":["### **Import NLTK library**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4409,"status":"ok","timestamp":1722039565528,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"jS1JYwnW-Fpd","outputId":"6009254a-6912-4354-b97f-c8d3009e0891"},"outputs":[{"name":"stdout","output_type":"stream","text":["3.8.1\n"]}],"source":["\n","import nltk\n","print(nltk.__version__)"]},{"cell_type":"markdown","metadata":{"id":"kBdVXjVfiHLA"},"source":["### **Import SpaCy library**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14630,"status":"ok","timestamp":1722039580151,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"kYt_mrMmLzok","outputId":"c7a98287-9ef3-4e36-c6c1-12ed5b0de80e"},"outputs":[{"name":"stdout","output_type":"stream","text":["3.7.5\n"]}],"source":["import spacy\n","print(spacy.__version__)"]},{"cell_type":"markdown","metadata":{"id":"34QP4orPjJaS"},"source":["## **Sentence tokenization**"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1315,"status":"ok","timestamp":1722039581411,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"jekJSBPwpWRZ","outputId":"61f7d1c0-5844-44a7-d4a5-122d745b1b5b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\aswathyr\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["['Tell me and I forget.',\n"," 'Teach me and I remember.',\n"," 'Involve me and I learn.']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["\n","import nltk\n","nltk.download('punkt') # Download the Punkt tokenizer models\n","from nltk.tokenize import sent_tokenize  # Import the sent_tokenize function from the nltk.tokenize module\n","\n","# Let us use a quote by Benjamin Franklin as an example\n","doc = \"\"\"Tell me and I forget. Teach me and I remember. Involve me and I learn.\"\"\"\n","\n","#Splitting the text into sentences\n","sent_tokenize(doc) # Tokenize the text into sentences"]},{"cell_type":"markdown","metadata":{"id":"DJ4iV1VvizPF"},"source":["#**Word Tokenization**"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1722039581412,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"x3WXmMQ_pgtt","outputId":"1f2b3cc5-d130-471d-bd23-e9555db5ed08"},"outputs":[{"data":{"text/plain":["['How', 'are', 'you', 'doing', '?']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import word_tokenize\n","# Let us use a simple sentence here\n","doc = \"\"\"How are you doing?\"\"\"\n","word_tokenize(doc) # Tokenize the text into words"]},{"cell_type":"markdown","metadata":{"id":"vJKDcFodjQiC"},"source":["## 3.8.2 SpaCy\n","\n","### **Tokenization**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2870,"status":"ok","timestamp":1722039584271,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"iDNRTy13vSdJ","outputId":"7d86ad12-bbbc-4b66-f1f9-75597ef21751"},"outputs":[{"name":"stdout","output_type":"stream","text":["How\n","are\n","you\n","doing\n","?\n"]}],"source":["import spacy\n","# Download the English model if not already installed\n","#!python -m spacy download en_core_web_sm \n","nlp = spacy.load('en_core_web_sm')\n","\n","# Create a Doc object\n","doc = nlp(u'How are you doing?')\n","\n","# Print each token separately\n","for token in doc:\n","    print(token.text)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1654,"status":"ok","timestamp":1722039585915,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"yRL-MhFM87Vc","outputId":"f1e499fd-a5b3-473c-c4fe-8039762a24c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tell me and I forget.\n","Teach me and I remember.\n","Involve me and I learn.\n"]}],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Create a Doc object\n","doc = nlp(u'Tell me and I forget. Teach me and I remember. Involve me and I learn.')\n","\n","# Print each token separately\n","for sent in doc.sents:\n","    print(sent.text)"]},{"cell_type":"markdown","metadata":{"id":"yECABsgzhtYM"},"source":["## **Stemming**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1722039585916,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"h3tSL1tZ7p8p","outputId":"710d78e9-d08f-4925-93fb-b8089a3e2abe"},"outputs":[{"name":"stdout","output_type":"stream","text":["connect\n"]}],"source":["from nltk.stem import PorterStemmer\n","porter = PorterStemmer()\n","print(porter.stem(\"connecting\"))\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2106,"status":"ok","timestamp":1722039588009,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"9_yZGzjVnSZj","outputId":"df212c6b-d5ff-4323-8672-e06a1f34e82d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Stemmed Word: connect\n"]}],"source":["import spacy\n","\n","# Load the English language model in Spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","# Word for stemming and lemmatization\n","word = \"connecting\"\n","# Create a single-token document\n","doc = nlp(word)\n","# Stemming using Spacy\n","stemmed_word = doc[0].lemma_\n","print(\"Stemmed Word:\", stemmed_word)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1722039588010,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"Jrl8WN5b9JWf","outputId":"84bc711c-9942-45f0-a063-874a6b6bcb31"},"outputs":[{"name":"stdout","output_type":"stream","text":["care\n","car\n"]}],"source":["from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","porter = PorterStemmer()# PorterStemmer\n","lancaster = LancasterStemmer()# LancasterStemmer\n","print(porter.stem(\"caring\"))\n","print(lancaster.stem(\"caring\"))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2267,"status":"ok","timestamp":1722039590262,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"nPNYdaeS_GQg","outputId":"d4e70b71-0775-45c7-f945-5df1a3b06743"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Word: caring\n","Stemmed Word: care\n","Lemmatized Word: care\n"]}],"source":["import spacy\n","\n","# Load the English language model in Spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Word for stemming and lemmatization\n","word = \"caring\"\n","\n","# Create a single-token document\n","doc = nlp(word)\n","\n","# Stemming using Spacy\n","stemmed_word = doc[0].lemma_\n","\n","# Lemmatization using Spacy\n","lemmatized_word = doc[0].lemma_ if doc[0].lemma_ != '-PRON-' else doc[0].text\n","\n","# Print the results\n","print(\"Original Word:\", word)\n","print(\"Stemmed Word:\", stemmed_word)\n","print(\"Lemmatized Word:\", lemmatized_word)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iPXwagnh-8lU"},"source":["## **Stop words**"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1722039590264,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"H6iOq4-FGRsV","outputId":"edbfd5b2-b319-499c-84e2-5e18506b6536"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\aswathyr\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping corpora\\stopwords.zip.\n"]},{"name":"stdout","output_type":"stream","text":["Stopwords in the sentence are: ['NLP', '?']\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\aswathyr\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","sentence = \"What is NLP?\" # Example sentence\n","words = word_tokenize(sentence) # Tokenize the sentence into words\n","stop_words = set(stopwords.words('english')) # Get the English stopwords from NLTK\n","filtered_words = [word for word in words if word.lower() not in stop_words] # Remove stopwords from the sentence\n","print(\"Stopwords in the sentence are:\", filtered_words) # Print the filtered words\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1722039590949,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"9vcc-gyAGW-5","outputId":"7b0f2c69-7424-41b1-867e-f61e12b93886"},"outputs":[{"name":"stdout","output_type":"stream","text":["Stopwords in the sentence are: ['NLP', '?']\n"]}],"source":["import spacy\n","\n","nlp = spacy.load('en_core_web_sm') # Load the English language model in spaCy\n","sentence = \"What is NLP?\" # Example sentence\n","doc = nlp(sentence) # Tokenize the sentence using spaCy\n","filtered_words = [token.text for token in doc if not token.is_stop] # Remove stopwords from the sentence\n","print(\"Stopwords in the sentence are:\", filtered_words) # Print the filtered words\n"]},{"cell_type":"markdown","metadata":{"id":"E3q-HMhcgkqI"},"source":["## Example.3.8.1 ##"]},{"cell_type":"markdown","metadata":{"id":"WpuaTfIRxaZM"},"source":["###Sentiment Analysis###\n","\n","Sentiment Analysis is a sub-field of Natural Language Processing (NLP) that tries to identify and extract the attitude, sentiments, evaluations, and emotions within a given text. It helps to determines whether data is positive, negative, or neutral. It has many applications in healthcare, customer service, banking, etc.\n","\n","In Python this can be implemented using VADER (Valence Aware Dictionary for Sentiment Reasoning) that is available in the NLTK package. It is a simple rule-based model for sentiment analysis that can efficiently handle vocabularies, abbreviations, capitalizations, repeated punctuations, emoticons, etc. VADER has the advantage of assessing the sentiment of any given text without the need for prior training. The result generated by VADER is a dictionary of 4 keys neg, neu, pos and compound. Here neg, neu, and pos means negative, neutral, and positive respectively. Their sum should be equal to 1 or close to it with float operation. The compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive). Using the compound score can be enough to determine the underlying sentiment of a text, because for:\n","\n","* a positive sentiment = compound ≥ 0.05\n","* a negative sentiment = compound ≤ -0.05\n","* a neutral sentiment = compound greater than -0.05 and less than 0.05.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1722039591226,"user":{"displayName":"Aswathy","userId":"06157770311800271718"},"user_tz":420},"id":"JG2_GGBcxQ-w","outputId":"73840afe-486f-4c3c-a1e0-9075c52d45ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["The hotel stay was horrible and uncomfortable. {'neg': 0.55, 'neu': 0.45, 'pos': 0.0, 'compound': -0.7269}\n","Always :) and be :D !------------------- {'neg': 0.0, 'neu': 0.291, 'pos': 0.709, 'compound': 0.8087}\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     C:\\Users\\aswathyr\\AppData\\Roaming\\nltk_data...\n"]}],"source":["import nltk\n","\n","# Download the lexicon\n","nltk.download(\"vader_lexicon\")\n","\n","# Import the lexicon\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","# Create an instance of SentimentIntensityAnalyzer\n","sent_analyzer = SentimentIntensityAnalyzer()\n","\n","def sentiment_analyzer_scores(sentence):\n","    score = sent_analyzer.polarity_scores(sentence)\n","    print(\"{:-<40} {}\".format(sentence, str(score)))\n","\n","sentiment_analyzer_scores(\"The hotel stay was horrible and uncomfortable.\")\n","sentiment_analyzer_scores(\"Always :) and be :D !\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
