{"cells":[{"cell_type":"markdown","source":["# **Example 6.2.3 (Multihead attention implementation)**\n","\n"," This tutorial is based on the following [link](https://medium.com/@fareedkhandev/create-gpt-from-scratch-using-python-part-1-bd89ccf6206a) and the detailed video link by Andrej Karpathy is here: [video link](https://youtu.be/kCc8FmEb1nY)"],"metadata":{"id":"mue2XtDsf-VC"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21910,"status":"ok","timestamp":1713305282626,"user":{"displayName":"Manel Martínez-Ramón","userId":"17325124127940578623"},"user_tz":360},"id":"aPGPTy6Gu18a","outputId":"41d8d9f5-943b-4eda-e231-5bff3f10aea2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"b83ac05d"},"source":["## Getting the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6530,"status":"ok","timestamp":1713305289154,"user":{"displayName":"Manel Martínez-Ramón","userId":"17325124127940578623"},"user_tz":360},"id":"85b595ed","outputId":"ec3d251b-85e4-4146-813c-2a8f104a5e20"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# import libraries\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"DiSGuojdHgOJ"},"source":["\n","\n","For the ease of understanding the concept we are using random data as the input.\n","\n","Note that, here\n","* _B_ is the batch size\n","*  _T_ is the token size\n","* _C_ corresponds to channel"]},{"cell_type":"markdown","metadata":{"id":"DyxiE2PXHgOU"},"source":["### Building a self attention head from scratch\n","\n","Notes:\n","- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n","\n","- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to **positionally encode** tokens.\n","\n","- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n","\n","\n","Here it is assumed that the future nodes doesn't communicate with the past. This is made sure by using the triangular matrix \"tril\". But we might have application where the communication might be required to happen across all the nodes. For example, sentiment classification where you need to understand the context of the entire sentence which means you need to communicate all the ways.\n","\n","- In an \"encoder\" attention block just remove the masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n","\n","- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n","\n","Here in the code block you can see :\n","```python\n","k = key(x)   \n","q = query(x)\n","v = value(x)\n","```\n","all the q,k,v comes from x.\n","\n","\n","**Cross attention** : q comes from x but k and v might come from some external blocks. For example, in transformers in decoder block query comes from x, however, k and v comes from the encoder block or the adjacent nodes. We are basically reading infromation from the side. In this scenarios, we like to get information from external source.\n","\n","- \"Scaled\" attention additional divides `wei` below by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1713305289410,"user":{"displayName":"Manel Martínez-Ramón","userId":"17325124127940578623"},"user_tz":360},"id":"-3x2q7eVMS3x","outputId":"83b1caa4-e8db-4390-cb4e-3203a7aa698d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n","        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n","        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n","        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n","       grad_fn=<SelectBackward0>)\n"]}],"source":["# version 4: self-attention with value!\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)\n","query = nn.Linear(C, head_size, bias=False)\n","value = nn.Linear(C, head_size, bias=False)\n","k = key(x)   # (B, T, 16)\n","q = query(x) # (B, T, 16)\n","\n","# all the queries will multiply with all the keys\n","wei = q @ k.transpose(-2,-1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T, T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","v = value(x)\n","out = wei @ v\n","#out = wei @ x\n","print(wei[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713305289410,"user":{"displayName":"Manel Martínez-Ramón","userId":"17325124127940578623"},"user_tz":360},"id":"FY22cV7-rvAy","outputId":"7fee9d25-2cab-48aa-caf3-c1a145b37838"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 8, 16])\n","torch.Size([4, 8, 8])\n"]}],"source":["print(v.size())\n","print(wei.size())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713305289411,"user":{"displayName":"Manel Martínez-Ramón","userId":"17325124127940578623"},"user_tz":360},"id":"0-Rdj2-pgdA6","outputId":"9477229e-2a1d-4435-8c6e-3836e26c4194"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0., 0., 0., 0.],\n","        [1., 1., 1., 0., 0., 0., 0., 0.],\n","        [1., 1., 1., 1., 0., 0., 0., 0.],\n","        [1., 1., 1., 1., 1., 0., 0., 0.],\n","        [1., 1., 1., 1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"]}],"source":["print(tril)"]},{"cell_type":"markdown","metadata":{"id":"A0229l42HgOQ"},"source":["\n","\n","Now let's look at the above code: instead of performing just an average of information from previous token and current token, we are using a triangular matrix to control the flow of information.\n","\n","This lower triangular matrix helps mask out the weights that we create. Then we normalize it. Once we apply infinities to the other masked regions and apply softmax, we get a uniform matrix.\n","\n","We don't want it to be all Uniform. Maybe we are looking for some interesting relationships which can be translated into this _wei_ matrix. For example, a vowel character might be more interested in the consonant character from the past. This information needs to flow from the past to the present in a data dependent manner.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7UemHrycPjKw"},"source":["### Query and Key\n","\n","Every single token at each position emit two vectors: Query (Q) and Key (V).\n","- Query (Q) : what am I looking for?\n","- Key (K) : What do I contain?\n","\n","The way we get the affinities between these two vectors is to perform a dot product of Q with K. This dot product would be _wei_ as this gives information about that specific token with respect to any other token in the sequence. For example, if the query and the key are well aligned then the _wei_ would interact to high amount which makes sure the right importance is given to these tokens based on Q and K. Lets implement this."]},{"cell_type":"markdown","metadata":{"id":"Ria1HGJ3P5IP"},"source":["Lets understand the importnace of query and key using an example:\n","\n","Take the last element of _wei[0]_, we can see that it is **0.2391**. So basically it is trying to communicate with a **query**, say for example:\n","\n","**_I am a vowel (it knows what content it has) at the 8th position (it knows its position too) and I need some information related to consonants till 4th position._**\n","\n","All the nodes would then emit **keys**. One of the channel would be saying that \"I am a consonant and I am occuring in a position within the 4th position of past token\" that key will have higher number in the channel. So the dot product of Q and K will have higher affinity in this case. So say when they have a higher value, for example, in this case highlighted in yellow in the above figure. So, since it has a higher value, lot of information will be aggregated to this particular position and hence learn a lot about it.\n","\n","This is what the _wei_ represents. Hence, through this matrix the sentence context is being learnt. It says how much of information needs to be aggregated from the tokens in the past."]},{"cell_type":"markdown","metadata":{"id":"H8aF53hnQjLN"},"source":["## Self attention head\n","Now lets create a self-attention head class out of the above code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rm4s1JFSQiz1"},"outputs":[],"source":["class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout) #This is to increase performance\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"qUMEzAvAQ8rE"},"source":["In the above class we have just implemented a single scaled dot-product self attention. Now lets see how we can add a multihead attention block.\n","\n","**Multihead attention**: It is just applying multiple self-attentions in parallel and concatenating the results.\n","\n","Basically this is like group convolution so instead of doing 1 big convolution operation we are splitting into n convolutions and adding them or concatenating them. So if we have $n_{embd} = 32$, we are just splitting the self-attention into 4 heads whioch would give 8 dimensional self-attention. Earlier we had 1 communication channel. But now we have 4 communication channel. Since we have 4 communication channel we now want 8-dimensional self attention to get the 32 number.\n","\n","So keeping this in mind we can implement the multihead attention class like below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAxkj_7rQ8NG"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"clYb8NOaSE_X"},"source":["Here we are implementing multihead attention in a BigramLanguage model which is a basic model thjat predicts each words probability based on the previous word. Bigrams means the pair of consecutive words. The class for the Bigram Language model is implemented below.\n","\n","Note that, we are adding token embedding layer and positional encoding layer as they are crucial to give much broader information related to the tokens. Also, the multihead attention layer is used here with 4 heads."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bddwORjFR9q1"},"outputs":[],"source":["class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.sa_head = MultiHeadAttention(4,n_embd//4) # i.e., 4 heads of 8-dimensional self-attention\n","        self.lm_head = nn.Linear(n_embd,vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B,T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        token_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = token_emb + pos_emb # both position identities and token identities #(B,T,C)\n","        x = self.sa_head(x)    # apply one head of self-attention, (B,T,C)\n","        x2=x\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","\n","        return logits, loss, x2\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss, x = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"]},{"cell_type":"markdown","metadata":{"id":"OMVYGqH1TONs"},"source":["We are using a \"poem.txt\" file to perform this experiment. Feel free to replace it with other text files to explore more related to the results.\n","\n","We are defining the batch size as 32 and the maximum context length for prediciton as 8 (block_size). number of embedding layer = 32. The iterations for which the model runs is 10000.\n","\n","* ### Read the input file and perform Tokenization\n","Once we read the input text file, each of the unique characters is identified to help with tokenization. A simple tokenization is done using the number of unique characters in the text file.\n","\n","* ### Train-test splitting\n","The tokenized data is then split into train and test groups.\n","\n","* ### Breaking data into chunks/ blocks:\n","We use _get_batch(split)_ function to generate small batches of data of inputs x and targets y using batch size and block size\n","\n","* ### Estimate loss\n","This function is used to estimate the loss during training and test.\n","\n","* ### Model training\n","Next we create the model and intialize the _AdamW_ optimizer. We use the estimate loss function and the optimizer to optimize and evaluate the loss on train and val sets.\n","\n","* ### Evaluation using model\n","Once we have the model trained, it is used to decode on test set and generate around 500 tokens.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snktBwcUR9q2"},"outputs":[],"source":["# hyperparameters\n","batch_size = 32 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","max_iters = 50500\n","eval_interval = 10000\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 32\n","n_head = 4\n","#n_layer = 6\n","dropout = 0.2\n","\n","# ------------\n","torch.manual_seed(1337)\n","\n","with open('/content/drive/MyDrive/DL_Book_Notebooks/Chapter 6: Attention Networks and transformers/Data/Rime_ancient_poem2.txt', 'r', encoding = 'utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss(): #It computes the loss function as the CE between the actual and the predicted token\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss, x = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss, x = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","\n","\n","print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2dxd_amrIkD"},"outputs":[],"source":["xb.size()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3AnJdoTcxa9"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}