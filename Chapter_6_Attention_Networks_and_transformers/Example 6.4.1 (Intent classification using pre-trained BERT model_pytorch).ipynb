{"cells":[{"cell_type":"markdown","source":["# **Example 6.4.1 (Intent classification using pre-trained BERT model)**"],"metadata":{"id":"PjfJUiDdiEjq"}},{"cell_type":"markdown","metadata":{"id":"K2H4dUwu8_qk"},"source":["## Intent Classification using BERT ##\n","Intent classification refers to the task of determining the intention or purpose behind a given text or user query. It involves categorizing a piece of text into predefined classes or categories based on its intended meaning. It is a fundamental task in natural language understanding (NLU) and plays a crucial role in various applications, including chatbots, virtual assistants, customer support systems, and more.\n","\n","The ATIS (Airline Travel Information Systems) dataset is a widely used benchmark dataset for intent classification and slot filling in the field of NLU. It was collected from the travel domain and contains queries and their corresponding intents and slots.\n","\n","Here are some examples of intents and queries from the ATIS dataset:\n","\n","1. Intent: flight_time\n","   Query: \"What time does the flight from Boston to New York depart?\"\n","\n","2. Intent: flight_booking\n","   Query: \"I want to book a flight from San Francisco to Los Angeles.\"\n","\n","3. Intent: flight_status\n","   Query: \"Is my flight from Chicago to Denver delayed?\"\n","\n","4. Intent: flight_cost\n","   Query: \"How much does a ticket from Seattle to Houston cost?\"\n","\n","5. Intent: airport_information\n","   Query: \"What is the phone number for O'Hare International Airport?\"\n","\n","In the ATIS dataset, the intent represents the intention or purpose of the user query. The queries are user-generated sentences or questions related to airline travel information. The dataset also includes slot annotations, where each slot corresponds to a specific piece of information extracted from the query (e.g., source airport, destination airport, departure time, etc.). However, in this example, we are focusing on intent classification. It provides a realistic representation of the types of queries users might ask in the context of airline travel. Researchers and practitioners use this dataset to train and test models for intent classification, slot filling, and other related tasks in natural language processing."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3051,"status":"ok","timestamp":1688834053708,"user":{"displayName":"Meenu Ajith","userId":"05544515161511089785"},"user_tz":240},"id":"7GwU9lg8K7Ya","outputId":"ceb7d2eb-d907-491a-cf06-1b9ae74c52a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4216,"status":"ok","timestamp":1688834057921,"user":{"displayName":"Meenu Ajith","userId":"05544515161511089785"},"user_tz":240},"id":"TYbBZVwdcqI5","outputId":"4da6d7ba-2cd1-4c12-9646-8a84599c49ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1165,"status":"ok","timestamp":1688834059079,"user":{"displayName":"Meenu Ajith","userId":"05544515161511089785"},"user_tz":240},"id":"fSd1KKJdVGVY","outputId":"5b9c8a80-944f-43ca-a753-5b51b15f584f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train examples:\n","Sentence:  i want to fly from boston at 838 am and arrive in denver at 1110 in the morning\n","Label: atis_flight\n","\n","Sentence:  what flights are available from pittsburgh to baltimore on thursday morning\n","Label: atis_flight\n","\n","Sentence:  what is the arrival time in san francisco for the 755 am flight leaving washington\n","Label: atis_flight_time\n","\n","Sentence:  cheapest airfare from tacoma to orlando\n","Label: atis_airfare\n","\n","Sentence:  round trip fares from pittsburgh to philadelphia under 1000 dollars\n","Label: atis_airfare\n","\n","Test examples:\n","Sentence:  i would like to find a flight from charlotte to las vegas that makes a stop in st. louis\n","Label: atis_flight\n","\n","Sentence:  on april first i need a ticket from tacoma to san jose departing before 7 am\n","Label: atis_airfare\n","\n","Sentence:  on april first i need a flight going from phoenix to san diego\n","Label: atis_flight\n","\n","Sentence:  i would like a flight traveling one way from phoenix to san diego on april first\n","Label: atis_flight\n","\n","Sentence:  i would like a flight from orlando to salt lake city for april first on delta airlines\n","Label: atis_flight\n","\n"]}],"source":["import pandas as pd\n","\n","# Load the ATIS dataset from train and test CSV files\n","def load_atis_dataset(train_file_path, test_file_path):\n","    train_df = pd.read_csv(train_file_path, names=['intent','query'])\n","    test_df = pd.read_csv(test_file_path, names=['intent','query'])\n","\n","    train_sentences = train_df['query'].tolist()\n","    train_labels = train_df['intent'].tolist()\n","\n","    test_sentences = test_df['query'].tolist()\n","    test_labels = test_df['intent'].tolist()\n","\n","    return train_sentences, train_labels, test_sentences, test_labels\n","\n","# Set the file paths for the train and test CSV files\n","train_file_path = '/content/drive/MyDrive/DL_Book_Notebooks/Ch6/atis_intents_train.csv'\n","test_file_path = '/content/drive/MyDrive/DL_Book_Notebooks/Ch6/atis_intents_test.csv'\n","\n","# Load the ATIS dataset\n","train_sentences, train_labels, test_sentences, test_labels = load_atis_dataset(train_file_path, test_file_path)\n","\n","# Print some examples from the train dataset\n","print(\"Train examples:\")\n","for sentence, label in zip(train_sentences[:5], train_labels[:5]):\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Label: {label}\")\n","    print()\n","\n","# Print some examples from the test dataset\n","print(\"Test examples:\")\n","for sentence, label in zip(test_sentences[:5], test_labels[:5]):\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Label: {label}\")\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12620,"status":"ok","timestamp":1688834071694,"user":{"displayName":"Meenu Ajith","userId":"05544515161511089785"},"user_tz":240},"id":"FEZR-uNDcjEt","outputId":"7265116e-1554-4c0e-9c01-4e8b90ac9ec1"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import torch\n","from torch import nn\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# Load the BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=8)  # Assuming 8 intent classes\n","\n","# Tokenize and encode the sentences\n","train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n","test_encodings = tokenizer(test_sentences, truncation=True, padding=True)\n","\n","# Convert the encodings to PyTorch tensors\n","train_inputs = torch.tensor(train_encodings['input_ids'])\n","test_inputs = torch.tensor(test_encodings['input_ids'])\n","\n","\n","# Create a mapping of unique intent labels to numerical labels\n","unique_labels = list(set(train_labels))\n","label_map = {label: index for index, label in enumerate(unique_labels)}\n","\n","# Convert the train_labels to numerical labels\n","train_labels = [label_map[label] for label in train_labels]\n","\n","# Convert the test_labels to numerical labels\n","test_labels = [label_map[label] for label in test_labels]\n","\n","# Convert the train_labels and test_labels to tensors\n","train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","\n","# Create a PyTorch dataset\n","train_dataset = torch.utils.data.TensorDataset(train_inputs, train_labels)\n","test_dataset = torch.utils.data.TensorDataset(test_inputs, test_labels)\n","\n","# Define the training parameters\n","batch_size = 16\n","epochs = 5\n","learning_rate = 1e-5\n","\n","# Create a DataLoader for training and testing\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Set the device to GPU if available, otherwise use CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define the optimizer and loss function\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":264208,"status":"ok","timestamp":1688834335890,"user":{"displayName":"Meenu Ajith","userId":"05544515161511089785"},"user_tz":240},"id":"JBxnXwEucclu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0564498-74ae-4805-e85d-cc18bd7578a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","Train Loss: 199.2293 | Train Accuracy: 0.8244\n","Test Loss: 11.4113 | Test Accuracy: 0.9425\n","\n","Epoch 2/5\n","Train Loss: 52.4783 | Train Accuracy: 0.9617\n","Test Loss: 2.9591 | Test Accuracy: 0.9900\n","\n","Epoch 3/5\n","Train Loss: 23.1584 | Train Accuracy: 0.9857\n","Test Loss: 1.8841 | Test Accuracy: 0.9950\n","\n","Epoch 4/5\n","Train Loss: 12.4296 | Train Accuracy: 0.9948\n","Test Loss: 1.8042 | Test Accuracy: 0.9938\n","\n","Epoch 5/5\n","Train Loss: 6.9761 | Train Accuracy: 0.9979\n","Test Loss: 1.8635 | Test Accuracy: 0.9925\n","\n"]}],"source":["# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0\n","    train_correct = 0\n","\n","    for inputs, labels in train_loader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)[0]\n","        _, predicted = torch.max(outputs, 1)\n","        loss = loss_fn(outputs, labels)\n","        train_loss += loss.item()\n","        train_correct += (predicted == labels).sum().item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    train_accuracy = train_correct / len(train_dataset)\n","\n","    # Evaluation on the test set\n","    model.eval()\n","    test_loss = 0\n","    test_correct = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)[0]\n","            _, predicted = torch.max(outputs, 1)\n","            loss = loss_fn(outputs, labels)\n","            test_loss += loss.item()\n","            test_correct += (predicted == labels).sum().item()\n","\n","    test_accuracy = test_correct / len(test_dataset)\n","\n","    # Print training and test metrics for each epoch\n","    print(f'Epoch {epoch + 1}/{epochs}')\n","    print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}')\n","    print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}')\n","    print()"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}