{"cells":[{"cell_type":"markdown","metadata":{"id":"YMdjmgZXBhhw"},"source":["# **Example 6.3.1 (Text generation using a decoder -only transformer architecture)**\n","\n","We have built a Multihead attention module to read a text file containing a poem as shown in another example. Now we can see how that structure using a combination of multihead attention and Bigram language model can be further improved to give a better text generation output. Let us look into the added modules"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27385,"status":"ok","timestamp":1713952045339,"user":{"displayName":"Manel Martínez-Ramón","userId":"17325124127940578623"},"user_tz":360},"id":"gD0MSFF_hibt","outputId":"98222c64-7ce5-48f8-c254-72be3dbf89ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"1tkAiAl7B77K"},"source":["## Importing the libraries\n","\n","Let us import the required linraries here. Note that the code setup is same as that we have in the multihead attention example. We are adding more transformer components and increasing the model complexity. This helps the model to extract more relevant features in a rigorous manner leading to better result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9c5Lgn1Bb44"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n"]},{"cell_type":"markdown","metadata":{"id":"4CiFvmnECuaa"},"source":["We will be using the same Multihead class and self attention module as in example 6.2.2. Th eonly difference is the added projection layer which is a linear layer having the size of $n_{embd} \\times n_{embd}$. We also add additional dropout layers to help with the training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5XptPbahCxLz"},"outputs":[],"source":["class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size): #This has a linear transformation for query, key and value, a dropout module and a triangular matrix for the decoder\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x): #This computes the attention coefficients, applies a dropout and then computes the attention representation, which is named \"out\"\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size): #This constructs num_heads heads inside nn.ModileList. It uses the class Head above\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x): #This concatenates the heads\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"qtdu4pmsEq0t"},"source":["In the original transformer architecture there is a feed forward part which finally combines into a block that gets repeated again. It is a simple NN. **What does this mean?**\n","\n","So far we have not implemented the feed forward part. So in the model architecture, we directly got the output from the heads and computed the logits based on this. We didn't quite give some time for the model to think about what the information it has obtained from multiple heads. So this is done by the feedforward network. **I do not inderstand this either**\n","\n","Lets implement this. So here a small feedforward block is implemented with a linear layer followed by non-linearity using _ReLu_. Now in the model, we initialize the feedforward blocks to have n_embd nodes and the output of self-attention passes sequentially to feedforward part. Baasically, the feedforward part thinks on the data individually and the data was gathered from self-attention modules."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWb_01TMFrC_"},"outputs":[],"source":["class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, n_embd), # think on the data individually and the data was gathered from self-attention modules\n","            nn.ReLU(),\n","        )\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"markdown","metadata":{"id":"uGXvhI0rFwrV"},"source":["But according to the original paper on Transformers, the combination of Multihead attention and feedforward network runs multiple times. So for easing that operation we are creating a class below called _Block_ which will do the back to back multihead attention and feedforward multiple times. In this case we have $\\times 3$\n","\n","Also, since the model is pretty deep now there can be optimization issues due to this. Therefore, to make it work we will be looking into few optimizations that might help. There are two optimizations introduced that help with improving performance of this deep model.\n","\n","- skip connection\n","- layer Normalization\n","\n","Next we will implement the skip connections and layer normalization used in the paper.\n","\n","As you can see we have added a linear feedforward layer followed by nonlinearity and dropout. In the _block_ class we have the multihead attention layer folowed by the feedforward layer with the layer normalizations done after the multihead attention and feed foward layers. Further, the residual connections/skip connections are implemented similar to the original tranformer architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxn-VIu-Cuwj"},"outputs":[],"source":["class FeedFoward(nn.Module): #The ffd module is a linear layer, followed by a relu followed by a linear followed by a dropout\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x)) #Add and norm after multi-head\n","        x = x + self.ffwd(self.ln2(x)) #Add and norm after a feed-forward\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GDRUSFEGH3Zd"},"source":["## Bigram language model\n","\n","The model which we introduced in example 6.2.2 is used here. Note that the model has been modified to incorporate the feedforward layer and the layer normalization. Also, another hyperparameter called **n_layer** is introduced here which corresponds to the number of repetitions that the block goes through back to back."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhkLoUbaDQX7"},"outputs":[],"source":["# super simple bigram model\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx"]},{"cell_type":"markdown","metadata":{"id":"n4INX527Cc7m"},"source":["## Define the hyperparameters\n","\n","Now let us define all the hyperparameters needed for the code to run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_XdqHJXCkj0"},"outputs":[],"source":["# hyperparameters\n","batch_size = 64 # how many independent sequences will we process in parallel?\n","block_size = 256 # what is the maximum context length for predictions?\n","max_iters = 2001\n","eval_interval = 10\n","learning_rate = 5e-5\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 384\n","n_head = 6\n","n_layer = 6\n","dropout = 0.2\n","# ------------"]},{"cell_type":"markdown","metadata":{"id":"iYiZIh-AJDBi"},"source":["We use the sam _poem.txt_ dataset as before for performing the experiment. The setup used is the same. lets see the performance of this model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ee55c7ocBZwU","outputId":"be0ffccd-8a41-43dc-994b-19da7e5779af"},"outputs":[{"output_type":"stream","name":"stdout","text":["10.762014 M parameters\n","step 0: train loss 3.5376, val loss 3.5295\n","step 10: train loss 2.7569, val loss 2.7632\n"]}],"source":["loss1 = []\n","loss2_tr = []\n","loss2_val = []\n","torch.manual_seed(1337)\n","\n","with open('/content/drive/MyDrive/DL_Book_Notebooks/Chapter 6: Attention Networks and transformers/Data/Rime_ancient_poem2.txt', 'r', encoding = 'utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        loss2_tr.append(losses['train'])\n","        loss2_val.append(losses['val'])\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    loss1.append(loss)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7LSSnfbgDQB"},"outputs":[],"source":["print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9y3dfuwTe_0t"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(torch.tensor(loss1))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}