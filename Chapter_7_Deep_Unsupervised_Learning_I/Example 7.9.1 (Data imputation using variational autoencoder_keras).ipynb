{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# Generate a synthetic dataset with missing values\n","np.random.seed(0)\n","original_data = np.random.randn(100, 1)\n","missing_mask = np.random.choice([0, 1], size=original_data.shape, p=[0.2, 0.8])\n","data_with_missing = original_data * missing_mask\n","\n","# Define the VAE model for data imputation\n","latent_dim = 2\n","\n","# Encoder\n","encoder_inputs = keras.Input(shape=(1,))\n","x = layers.Dense(32, activation=\"relu\")(encoder_inputs)\n","z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n","z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n","z = layers.Lambda(lambda args: tf.random.normal(shape=(tf.shape(args[0])[0], latent_dim)) * tf.exp(0.5 * args[1]) + args[0], name=\"z\")([z_mean, z_log_var])\n","encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n","\n","# Decoder\n","latent_inputs = keras.Input(shape=(latent_dim,))\n","x = layers.Dense(32, activation=\"relu\")(latent_inputs)\n","outputs = layers.Dense(1)(x)\n","decoder = keras.Model(latent_inputs, outputs, name=\"decoder\")\n","\n","# VAE\n","outputs = decoder(encoder(encoder_inputs)[2])\n","vae = keras.Model(encoder_inputs, outputs, name=\"vae\")\n","\n","# Define VAE loss function\n","def vae_loss(encoder_inputs, outputs, z_mean, z_log_var):\n","    reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n","    reconstruction_loss = reconstruction_loss_fn(encoder_inputs, outputs)\n","    kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n","    return reconstruction_loss + 0.1 * kl_loss  # Adjust the weight of the KL divergence term\n","\n","# Custom training step\n","optimizer = keras.optimizers.Adam()\n","\n","@tf.function\n","def train_step(data):\n","    with tf.GradientTape() as tape:\n","        z_mean, z_log_var, z = encoder(data)\n","        reconstructed = decoder(z)\n","        loss = vae_loss(data, reconstructed, z_mean, z_log_var)\n","    gradients = tape.gradient(loss, vae.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n","    return loss\n","\n","# Training loop\n","epochs = 100\n","batch_size = 32\n","dataset = tf.data.Dataset.from_tensor_slices(data_with_missing).batch(batch_size)\n","\n","for epoch in range(epochs):\n","    for step, batch_data in enumerate(dataset):\n","        loss = train_step(batch_data)\n","    print(f\"Epoch {epoch + 1}, Loss: {loss.numpy()}\")\n","\n","# Perform data imputation\n","imputed_data = vae.predict(data_with_missing)\n","print(\"Original Data\\tData with Missing Values\\tImputed Data\")\n","for original, missing, imputed in zip(original_data[:10], data_with_missing[:10], imputed_data[:10]):\n","    print(f\"{original}\\t{missing}\\t{imputed}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dAH650Jia6tZ","executionInfo":{"status":"ok","timestamp":1723245284872,"user_tz":420,"elapsed":27919,"user":{"displayName":"Aswathy","userId":"06157770311800271718"}},"outputId":"d04fa221-e9c4-4db5-809e-0c4d00d2ff7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: [0.7795422  0.8250585  0.77983063 0.7819766 ]\n","Epoch 2, Loss: [0.8331166  0.88270205 0.83352643 0.8360044 ]\n","Epoch 3, Loss: [0.8654649  0.9195336  0.86602896 0.8688874 ]\n","Epoch 4, Loss: [1.3295064 1.388175  1.3302343 1.3334875]\n","Epoch 5, Loss: [0.9485084  1.0117984  0.94939595 0.95305884]\n","Epoch 6, Loss: [1.4464006 1.5141566 1.4474492 1.4515238]\n","Epoch 7, Loss: [0.6750906  0.74808997 0.6763172  0.68086594]\n","Epoch 8, Loss: [0.70230484 0.7811089  0.70371675 0.7087934 ]\n","Epoch 9, Loss: [0.76284933 0.8476394  0.76444227 0.7700832 ]\n","Epoch 10, Loss: [0.8820605 0.9729386 0.8838269 0.8900554]\n","Epoch 11, Loss: [0.8268822  0.92397124 0.8288248  0.8356193 ]\n","Epoch 12, Loss: [0.90702647 1.0099869  0.9091257  0.91645557]\n","Epoch 13, Loss: [0.82602656 0.9352707  0.8282859  0.836185  ]\n","Epoch 14, Loss: [0.93692356 1.0520582  0.9393321  0.94777375]\n","Epoch 15, Loss: [0.40103376 0.52179044 0.40357444 0.41255355]\n","Epoch 16, Loss: [0.45780236 0.5837267  0.4604535  0.46992877]\n","Epoch 17, Loss: [0.43725625 0.5683025  0.44000328 0.4499752 ]\n","Epoch 18, Loss: [0.5526049  0.6897818  0.55545765 0.5660151 ]\n","Epoch 19, Loss: [0.73343074 0.87699693 0.7363798  0.747549  ]\n","Epoch 20, Loss: [0.4995926  0.6502545  0.50266004 0.51450855]\n","Epoch 21, Loss: [0.28382412 0.44217908 0.28700346 0.29955557]\n","Epoch 22, Loss: [0.44822082 0.614406   0.45149687 0.4647833 ]\n","Epoch 23, Loss: [0.6188837  0.79412025 0.6222714  0.6364061 ]\n","Epoch 24, Loss: [0.19728494 0.38324273 0.2008065  0.21592848]\n","Epoch 25, Loss: [0.34301552 0.54030746 0.34667385 0.36284503]\n","Epoch 26, Loss: [0.33610043 0.54540443 0.33992395 0.35722774]\n","Epoch 27, Loss: [0.33012003 0.5515523  0.3340991  0.35257596]\n","Epoch 28, Loss: [0.6032345 0.8363708 0.6073325 0.6269535]\n","Epoch 29, Loss: [1.0121155 1.2571279 1.0163213 1.0370457]\n","Epoch 30, Loss: [0.69599855 0.95378125 0.7003408  0.72226584]\n","Epoch 31, Loss: [0.32551223 0.59720004 0.33001953 0.35327205]\n","Epoch 32, Loss: [0.2593107  0.5452864  0.26398453 0.28861234]\n","Epoch 33, Loss: [0.97006917 1.2709647  0.9749233  1.0009834 ]\n","Epoch 34, Loss: [0.59336066 0.909649   0.5983631  0.62584007]\n","Epoch 35, Loss: [0.30410117 0.63541627 0.30921894 0.33804545]\n","Epoch 36, Loss: [0.5036545  0.8480212  0.508826   0.53873247]\n","Epoch 37, Loss: [0.14159584 0.49867046 0.14678943 0.17770977]\n","Epoch 38, Loss: [0.12859538 0.49990794 0.13385566 0.16592109]\n","Epoch 39, Loss: [0.09772892 0.48206544 0.10302188 0.13610986]\n","Epoch 40, Loss: [0.29076996 0.68555486 0.29605466 0.3299323 ]\n","Epoch 41, Loss: [0.16573295 0.57306063 0.17108701 0.20599082]\n","Epoch 42, Loss: [0.5400644  0.96029484 0.54553556 0.5815798 ]\n","Epoch 43, Loss: [0.23902947 0.67144907 0.24463472 0.2818124 ]\n","Epoch 44, Loss: [0.50167793 0.94605863 0.5073439  0.5455412 ]\n","Epoch 45, Loss: [0.9567553 1.4167691 0.9626332 1.0022434]\n","Epoch 46, Loss: [0.18759339 0.66304225 0.19372305 0.23473567]\n","Epoch 47, Loss: [0.05150319 0.540299   0.05777069 0.09988516]\n","Epoch 48, Loss: [0.09855394 0.59580725 0.10483256 0.14754975]\n","Epoch 49, Loss: [0.15619175 0.6609922  0.16254057 0.20587738]\n","Epoch 50, Loss: [0.01300191 0.5264074  0.0194969  0.06360396]\n","Epoch 51, Loss: [0.5346829  1.0539689  0.54118025 0.5857015 ]\n","Epoch 52, Loss: [0.13185945 0.65514416 0.13824472 0.18289919]\n","Epoch 53, Loss: [0.12032746 0.6492761  0.1266532  0.171617  ]\n","Epoch 54, Loss: [0.18621467 0.7213412  0.19258544 0.2379813 ]\n","Epoch 55, Loss: [0.170367   0.7085803  0.17678705 0.2223826 ]\n","Epoch 56, Loss: [0.11000113 0.6512997  0.11654265 0.16243263]\n","Epoch 57, Loss: [0.38540778 0.9292149  0.3920508  0.4381871 ]\n","Epoch 58, Loss: [0.29452187 0.8406321  0.3012445  0.34757555]\n","Epoch 59, Loss: [0.06716411 0.61338776 0.07387046 0.12016192]\n","Epoch 60, Loss: [0.547151   1.0923843  0.55384725 0.6000269 ]\n","Epoch 61, Loss: [0.12098157 0.6666451  0.12780118 0.17409344]\n","Epoch 62, Loss: [0.14299195 0.68866813 0.14990279 0.19624601]\n","Epoch 63, Loss: [0.28652108 0.8318094  0.2934735  0.33978727]\n","Epoch 64, Loss: [0.27952504 0.82211167 0.28628302 0.3321823 ]\n","Epoch 65, Loss: [0.08726019 0.6257371  0.09378321 0.13913785]\n","Epoch 66, Loss: [0.08989377 0.6242964  0.096304   0.14116652]\n","Epoch 67, Loss: [0.04625297 0.5762465  0.05270158 0.09719715]\n","Epoch 68, Loss: [0.12364358 0.64721006 0.13007823 0.1740173 ]\n","Epoch 69, Loss: [0.24519166 0.7623533  0.2515375  0.29483968]\n","Epoch 70, Loss: [0.11375674 0.623941   0.12001729 0.16265969]\n","Epoch 71, Loss: [0.15836014 0.6640486  0.1645955  0.20681761]\n","Epoch 72, Loss: [0.1521462  0.65493155 0.15845846 0.20045522]\n","Epoch 73, Loss: [0.19653888 0.6996977  0.20302328 0.24518782]\n","Epoch 74, Loss: [0.11792442 0.62165207 0.12449052 0.16676965]\n","Epoch 75, Loss: [0.15659036 0.66281474 0.1633152  0.20587695]\n","Epoch 76, Loss: [0.15845744 0.6665374  0.16538455 0.20821406]\n","Epoch 77, Loss: [0.205385   0.71228397 0.21232677 0.25501975]\n","Epoch 78, Loss: [0.16395319 0.6686724  0.17085269 0.21326058]\n","Epoch 79, Loss: [0.34761807 0.8489306  0.35443795 0.39638343]\n","Epoch 80, Loss: [0.11713519 0.6176442  0.123959   0.16572537]\n","Epoch 81, Loss: [0.2704151  0.7646721  0.27703902 0.3181233 ]\n","Epoch 82, Loss: [0.17907552 0.6690904  0.18565509 0.22633743]\n","Epoch 83, Loss: [0.04674957 0.53259236 0.05323822 0.09348518]\n","Epoch 84, Loss: [0.31363344 0.7954718  0.32016668 0.3600838 ]\n","Epoch 85, Loss: [0.17593952 0.65273577 0.18246603 0.22195846]\n","Epoch 86, Loss: [0.19687083 0.66773635 0.20332053 0.2422613 ]\n","Epoch 87, Loss: [0.17041345 0.6346729  0.17666978 0.21492064]\n","Epoch 88, Loss: [0.47208408 0.932372   0.4781502  0.5158894 ]\n","Epoch 89, Loss: [0.23179068 0.6907239  0.23771347 0.2752097 ]\n","Epoch 90, Loss: [0.13250655 0.594236   0.13842703 0.17616767]\n","Epoch 91, Loss: [0.10145605 0.5658614  0.10740451 0.14543524]\n","Epoch 92, Loss: [0.0982452  0.5637081  0.10411125 0.14220166]\n","Epoch 93, Loss: [0.2636921  0.72528976 0.2693741  0.3070658 ]\n","Epoch 94, Loss: [0.15270726 0.6120206  0.15837562 0.1959039 ]\n","Epoch 95, Loss: [0.0985307  0.5572361  0.10433502 0.14193103]\n","Epoch 96, Loss: [0.1097836  0.5660292  0.11562506 0.15307061]\n","Epoch 97, Loss: [0.1822604  0.6343262  0.18806478 0.2251755 ]\n","Epoch 98, Loss: [0.0482762  0.49736235 0.05408968 0.09102672]\n","Epoch 99, Loss: [0.09222473 0.53722745 0.09811568 0.13486627]\n","Epoch 100, Loss: [0.04840324 0.48733354 0.05431821 0.09068426]\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","Original Data\tData with Missing Values\tImputed Data\n","[1.76405235]\t[1.76405235]\t[1.7015392]\n","[0.40015721]\t[0.40015721]\t[-0.00750341]\n","[0.97873798]\t[0.]\t[0.01855113]\n","[2.2408932]\t[2.2408932]\t[1.7848713]\n","[1.86755799]\t[1.86755799]\t[1.4728615]\n","[-0.97727788]\t[-0.97727788]\t[-0.8055777]\n","[0.95008842]\t[0.95008842]\t[0.90160435]\n","[-0.15135721]\t[-0.15135721]\t[-0.3438847]\n","[-0.10321885]\t[-0.]\t[-0.26320636]\n","[0.4105985]\t[0.4105985]\t[0.47405857]\n"]}]},{"cell_type":"code","source":["# Print some results\n","print(\"Original Data:\")\n","print(original_data[:10])\n","print(\"Data with Missing Values:\")\n","print(data_with_missing[:10])\n","print(\"Imputed Data:\")\n","print(imputed_data[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1sld7P0ZXSy","executionInfo":{"status":"ok","timestamp":1723245296712,"user_tz":420,"elapsed":226,"user":{"displayName":"Aswathy","userId":"06157770311800271718"}},"outputId":"71695a20-da66-440b-c974-e90c6909ee13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Data:\n","[[ 1.76405235]\n"," [ 0.40015721]\n"," [ 0.97873798]\n"," [ 2.2408932 ]\n"," [ 1.86755799]\n"," [-0.97727788]\n"," [ 0.95008842]\n"," [-0.15135721]\n"," [-0.10321885]\n"," [ 0.4105985 ]]\n","Data with Missing Values:\n","[[ 1.76405235]\n"," [ 0.40015721]\n"," [ 0.        ]\n"," [ 2.2408932 ]\n"," [ 1.86755799]\n"," [-0.97727788]\n"," [ 0.95008842]\n"," [-0.15135721]\n"," [-0.        ]\n"," [ 0.4105985 ]]\n","Imputed Data:\n","[[ 1.7015392 ]\n"," [-0.00750341]\n"," [ 0.01855113]\n"," [ 1.7848713 ]\n"," [ 1.4728615 ]\n"," [-0.8055777 ]\n"," [ 0.90160435]\n"," [-0.3438847 ]\n"," [-0.26320636]\n"," [ 0.47405857]]\n"]}]}]}