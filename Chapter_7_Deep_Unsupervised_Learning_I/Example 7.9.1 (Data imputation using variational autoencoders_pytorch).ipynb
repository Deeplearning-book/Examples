{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHIsyg6tCGkZXAzn6K4qXd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VS6HLre5fo9_","executionInfo":{"status":"ok","timestamp":1694540729027,"user_tz":240,"elapsed":8907,"user":{"displayName":"Meenu Ajith","userId":"05544515161511089785"}},"outputId":"724e3185-8095-44bd-c156-0fa3c1d2d70a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100] Loss: 0.9524822425842285\n","Epoch [2/100] Loss: 0.9198378562927246\n","Epoch [3/100] Loss: 0.9081756210327149\n","Epoch [4/100] Loss: 0.8561338877677918\n","Epoch [5/100] Loss: 0.8490874099731446\n","Epoch [6/100] Loss: 0.849484293460846\n","Epoch [7/100] Loss: 0.8794067668914795\n","Epoch [8/100] Loss: 0.8170731163024902\n","Epoch [9/100] Loss: 0.8352644538879395\n","Epoch [10/100] Loss: 0.8223301029205322\n","Epoch [11/100] Loss: 0.7549737393856049\n","Epoch [12/100] Loss: 0.770291531085968\n","Epoch [13/100] Loss: 0.768078688532114\n","Epoch [14/100] Loss: 0.7691821533441544\n","Epoch [15/100] Loss: 0.692395088672638\n","Epoch [16/100] Loss: 0.7302446037530899\n","Epoch [17/100] Loss: 0.7131495821475983\n","Epoch [18/100] Loss: 0.7065990990400315\n","Epoch [19/100] Loss: 0.6261398273706437\n","Epoch [20/100] Loss: 0.6398012076318264\n","Epoch [21/100] Loss: 0.627301561832428\n","Epoch [22/100] Loss: 0.6232863807678223\n","Epoch [23/100] Loss: 0.5686908796429634\n","Epoch [24/100] Loss: 0.5270124864578247\n","Epoch [25/100] Loss: 0.5405384069681167\n","Epoch [26/100] Loss: 0.5676921439170838\n","Epoch [27/100] Loss: 0.46444573879241946\n","Epoch [28/100] Loss: 0.5146674877405166\n","Epoch [29/100] Loss: 0.5040159630775451\n","Epoch [30/100] Loss: 0.4782871550321579\n","Epoch [31/100] Loss: 0.45222169935703277\n","Epoch [32/100] Loss: 0.44349697947502137\n","Epoch [33/100] Loss: 0.4651918542385101\n","Epoch [34/100] Loss: 0.4044842767715454\n","Epoch [35/100] Loss: 0.346176016330719\n","Epoch [36/100] Loss: 0.34515277177095416\n","Epoch [37/100] Loss: 0.39834036111831667\n","Epoch [38/100] Loss: 0.29441911458969117\n","Epoch [39/100] Loss: 0.29860994815826414\n","Epoch [40/100] Loss: 0.2855875062942505\n","Epoch [41/100] Loss: 0.2663445007801056\n","Epoch [42/100] Loss: 0.3188492822647095\n","Epoch [43/100] Loss: 0.30474075496196745\n","Epoch [44/100] Loss: 0.3022781801223755\n","Epoch [45/100] Loss: 0.2752414757013321\n","Epoch [46/100] Loss: 0.3050360906124115\n","Epoch [47/100] Loss: 0.2739611279964447\n","Epoch [48/100] Loss: 0.27562352299690246\n","Epoch [49/100] Loss: 0.29420839726924897\n","Epoch [50/100] Loss: 0.2765698099136353\n","Epoch [51/100] Loss: 0.25633479952812194\n","Epoch [52/100] Loss: 0.2598791235685349\n","Epoch [53/100] Loss: 0.2653910171985626\n","Epoch [54/100] Loss: 0.24296887159347536\n","Epoch [55/100] Loss: 0.2661785817146301\n","Epoch [56/100] Loss: 0.2521689820289612\n","Epoch [57/100] Loss: 0.2609665632247925\n","Epoch [58/100] Loss: 0.2694650900363922\n","Epoch [59/100] Loss: 0.28038538813591\n","Epoch [60/100] Loss: 0.246536066532135\n","Epoch [61/100] Loss: 0.26493507623672485\n","Epoch [62/100] Loss: 0.2291620510816574\n","Epoch [63/100] Loss: 0.24431639432907104\n","Epoch [64/100] Loss: 0.23516733527183534\n","Epoch [65/100] Loss: 0.2506459224224091\n","Epoch [66/100] Loss: 0.23131683588027954\n","Epoch [67/100] Loss: 0.25694656133651733\n","Epoch [68/100] Loss: 0.256725555062294\n","Epoch [69/100] Loss: 0.2307540476322174\n","Epoch [70/100] Loss: 0.2377425503730774\n","Epoch [71/100] Loss: 0.26479300737380984\n","Epoch [72/100] Loss: 0.2204571521282196\n","Epoch [73/100] Loss: 0.23114897131919862\n","Epoch [74/100] Loss: 0.22086995154619216\n","Epoch [75/100] Loss: 0.23863097310066222\n","Epoch [76/100] Loss: 0.21586346983909607\n","Epoch [77/100] Loss: 0.2216291296482086\n","Epoch [78/100] Loss: 0.22125685214996338\n","Epoch [79/100] Loss: 0.22823591649532318\n","Epoch [80/100] Loss: 0.22602496445178985\n","Epoch [81/100] Loss: 0.22429470658302308\n","Epoch [82/100] Loss: 0.20809445261955262\n","Epoch [83/100] Loss: 0.21468185424804687\n","Epoch [84/100] Loss: 0.23277486085891724\n","Epoch [85/100] Loss: 0.2242397779226303\n","Epoch [86/100] Loss: 0.21895628571510314\n","Epoch [87/100] Loss: 0.2189336621761322\n","Epoch [88/100] Loss: 0.22591652572155\n","Epoch [89/100] Loss: 0.23489151418209075\n","Epoch [90/100] Loss: 0.2217027199268341\n","Epoch [91/100] Loss: 0.22325653791427613\n","Epoch [92/100] Loss: 0.21929813861846925\n","Epoch [93/100] Loss: 0.22539210081100464\n","Epoch [94/100] Loss: 0.2062903380393982\n","Epoch [95/100] Loss: 0.20496354818344117\n","Epoch [96/100] Loss: 0.22662527441978456\n","Epoch [97/100] Loss: 0.21944714784622193\n","Epoch [98/100] Loss: 0.20319475173950197\n","Epoch [99/100] Loss: 0.20539188921451568\n","Epoch [100/100] Loss: 0.19924708604812622\n","Original Data:\n","[[ 1.76405235]\n"," [ 0.40015721]\n"," [ 0.97873798]\n"," [ 2.2408932 ]\n"," [ 1.86755799]\n"," [-0.97727788]\n"," [ 0.95008842]\n"," [-0.15135721]\n"," [-0.10321885]\n"," [ 0.4105985 ]]\n","Data with Missing Values:\n","[[ 1.7640524 ]\n"," [ 0.4001572 ]\n"," [ 0.        ]\n"," [ 2.2408931 ]\n"," [ 1.867558  ]\n"," [-0.9772779 ]\n"," [ 0.95008844]\n"," [-0.1513572 ]\n"," [-0.        ]\n"," [ 0.41059852]]\n","Imputed Data:\n","[[ 1.7711978 ]\n"," [ 0.55003905]\n"," [-0.12007252]\n"," [ 2.2874293 ]\n"," [ 1.9756222 ]\n"," [-1.151875  ]\n"," [ 0.9365829 ]\n"," [ 0.22888902]\n"," [-0.10971497]\n"," [ 0.03094632]]\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Generate a synthetic dataset with missing values\n","# For simplicity, we'll use a 1D dataset with some missing values\n","np.random.seed(0)\n","original_data = np.random.randn(100, 1)\n","missing_mask = np.random.choice([0, 1], size=original_data.shape, p=[0.2, 0.8])\n","data_with_missing = original_data * missing_mask\n","\n","# Define the VAE model for data imputation\n","latent_dim = 2\n","\n","# Encoder\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.fc1 = nn.Linear(1, 32)\n","        self.fc_mean = nn.Linear(32, latent_dim)\n","        self.fc_log_var = nn.Linear(32, latent_dim)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        mean = self.fc_mean(x)\n","        log_var = self.fc_log_var(x)\n","        return mean, log_var\n","\n","# Decoder\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        self.fc1 = nn.Linear(latent_dim, 32)\n","        self.fc2 = nn.Linear(32, 1)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# VAE\n","class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","\n","    def forward(self, x):\n","        mean, log_var = self.encoder(x)\n","        std = torch.exp(0.5 * log_var)\n","        epsilon = torch.randn_like(std)\n","        z = mean + epsilon * std\n","        reconstructed = self.decoder(z)\n","        return reconstructed, mean, log_var\n","\n","# Define VAE loss function\n","def vae_loss(reconstructed, x, mean, log_var):\n","    mse_loss = nn.functional.mse_loss(reconstructed, x, reduction='sum')\n","    kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n","    return mse_loss + 0.1 * kl_loss  # You can adjust the weight of the KL divergence term\n","\n","# Convert data to PyTorch tensors\n","data_with_missing = torch.tensor(data_with_missing, dtype=torch.float32)\n","\n","# Create DataLoader\n","batch_size = 32\n","data_loader = DataLoader(TensorDataset(data_with_missing), batch_size=batch_size, shuffle=True)\n","\n","# Initialize the VAE model and optimizer\n","vae = VAE()\n","optimizer = optim.Adam(vae.parameters())\n","\n","# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    for batch in data_loader:\n","        optimizer.zero_grad()\n","        x = batch[0]\n","        reconstructed, mean, log_var = vae(x)\n","        loss = vae_loss(reconstructed, x, mean, log_var)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {total_loss / len(data_with_missing)}\")\n","\n","# Perform data imputation\n","imputed_data = vae(data_with_missing)[0].detach().numpy()\n","\n","# Print some results\n","print(\"Original Data:\")\n","print(original_data[:10])\n","print(\"Data with Missing Values:\")\n","print(data_with_missing[:10].detach().numpy())\n","print(\"Imputed Data:\")\n","print(imputed_data[:10])\n"]}]}